{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd75f1d-7804-4644-a61d-3a0682093fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8af36d8-b509-4f14-ad2e-12253e37689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original local endpoints (commented out)\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base_list = [\n",
    "   \"http://localhost:8000/v1\",\n",
    "   # \"http://44.247.43.195:18901/v1\",\n",
    "   # os.environ.get(\"LLM_AS_A_JUDGE_BASE\", \"http://10.39.3.123:18901/v1\"),\n",
    "]\n",
    "\n",
    "client_list = []\n",
    "for api_base in openai_api_base_list:\n",
    "    client = OpenAI(\n",
    "       api_key=openai_api_key,\n",
    "       base_url=api_base,\n",
    "    )\n",
    "    client_list.append(client)\n",
    "    model_name_list = []\n",
    "for client in client_list:\n",
    "    response = requests.get(f\"{api_base}/models\")\n",
    "    models = response.json()\n",
    "    model_name_list.append(models['data'][0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41745796-3263-4dd4-8b9c-5c5c49d50ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x1551264edf30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69eed7b2-7d10-41c3-8c08-8c5480afd771",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 336) (vl_agent.py, line 336)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/hits/code/renly/conda_env/verl/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\n\u001b[0;31m    from vl_agent import (\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/nfs/mm-isilon/brainscans/dropbox/code/xinhaih/DeepEyes/verl/utils/reward_score/vl_agent.py:336\u001b[0;36m\u001b[0m\n\u001b[0;31m    - **0.25**: Attempts basic image processing (e.g., cropping) but implementation is flawed or doesn't help. Or gives answer with very limited explanation.\u001b[0m\n\u001b[0m                                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 336)\n"
     ]
    }
   ],
   "source": [
    "from vl_agent import (\n",
    "    rubrics_judge,\n",
    "    extract_answer,\n",
    "    RUBRIC_VS_USAGE,\n",
    "    RUBRICS_JUDGE_SYSTEM_PROMPT_VS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2361e-e99a-4e06-904d-676641f055f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path\n",
    "IMAGE_PATH = \"/workspace/shared/datasets/CodeV_images/45222.jpg\"\n",
    "\n",
    "\n",
    "def test_vs_score_1_0():\n",
    "    \"\"\"VS: Perfect score - proper cropping and color analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEST 1: VS - Score 1.0 (Proper cropping + analysis)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    question = \"what's the color of vase\"\n",
    "    ground_truth = \"blue\"\n",
    "\n",
    "    response = \"\"\"<think>I need to locate the vase in the image and analyze its color. Let me crop the vase region to get a better view.</think>\n",
    "<code>\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open('45222.jpg')\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Crop vase region (estimated coordinates)\n",
    "vase_crop = img.crop((200, 200, 500, 400))\n",
    "vase_crop.save('vase_region.jpg')\n",
    "\n",
    "# Analyze dominant color in cropped region\n",
    "vase_array = np.array(vase_crop)\n",
    "avg_color = vase_array.mean(axis=(0,1))\n",
    "print(f\"Average RGB: {avg_color}\")\n",
    "\n",
    "# Blue channel is dominant\n",
    "if avg_color[2] > avg_color[0] and avg_color[2] > avg_color[1]:\n",
    "    color = \"blue\"\n",
    "print(f\"Vase color: {color}\")\n",
    "</code>\n",
    "<sandbox_output>Average RGB: [45.2 78.3 156.8]\n",
    "Vase color: blue</sandbox_output>\n",
    "<answer>blue</answer>\"\"\"\n",
    "\n",
    "    extracted = extract_answer(response)\n",
    "\n",
    "    # Create mock PIL images\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        img = Image.open(IMAGE_PATH)\n",
    "        mock_images = [img.crop((200, 200, 500, 400))]\n",
    "    except:\n",
    "        mock_images = None\n",
    "\n",
    "    score = rubrics_judge(response, question, ground_truth, RUBRIC_VS_USAGE,\n",
    "                          extracted, RUBRICS_JUDGE_SYSTEM_PROMPT_VS, mock_images, IMAGE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "verl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
