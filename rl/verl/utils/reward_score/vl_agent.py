from openai import OpenAI
import requests
import random
import re
import os
import base64
from io import BytesIO

from math_verify import parse, verify

# Local judge endpoint
openai_api_base = os.environ.get("LLM_AS_A_JUDGE_BASE", "http://localhost:18901/v1")
openai_api_key = os.environ.get("VLLM_API_KEY", "EMPTY")

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)
client_list = [client]

# Get model name from vLLM server
try:
    headers = {"Authorization": f"Bearer {openai_api_key}"} if openai_api_key != "EMPTY" else {}
    response = requests.get(f"{openai_api_base}/models", headers=headers, timeout=5)
    models = response.json()
    model_name = models['data'][0]['id']
    model_name_list = [model_name]
    print(f"[INFO] Using judge at {openai_api_base}, model: {model_name}")
except Exception as e:
    print(f"[WARNING] Could not fetch model name from judge at {openai_api_base}: {e}")
    model_name_list = ["judge"]  # Fallback

# Uncomment below to use OpenAI instead
# openai_api_key = os.environ.get("OPENAI_API_KEY")
# client_list = [OpenAI(api_key=openai_api_key)]
# model_name_list = ["gpt-5-mini"]


def _encode_image_to_base64(image):
    """
    Encode a PIL Image to base64 string.

    Args:
        image: PIL Image or file path

    Returns:
        str: Base64 encoded image string
    """
    from PIL import Image

    # Handle file path
    if isinstance(image, str):
        if not os.path.exists(image):
            print(f"[WARNING] Image file not found: {image}")
            return None
        try:
            image = Image.open(image)
        except Exception as e:
            print(f"[WARNING] Failed to open image {image}: {e}")
            return None

    # Handle PIL Image
    if image is None:
        return None

    try:
        buffer = BytesIO()
        # Convert to RGB if needed (handles RGBA, grayscale, etc.)
        if image.mode not in ('RGB', 'L'):
            image = image.convert('RGB')
        image.save(buffer, format='JPEG', quality=85)
        buffer.seek(0)
        img_bytes = buffer.getvalue()
        encoded = base64.b64encode(img_bytes).decode('utf-8')
        return encoded
    except Exception as e:
        print(f"[WARNING] Failed to encode image: {e}")
        return None


def _prepare_multimodal_input(text_prompt, generated_images=None, input_image_path=None,
                              question=None, response_text=None, max_images=3):
    """
    Prepare multimodal input for OpenAI Vision API with better organization.

    Args:
        text_prompt: Evaluation prompt (Agent Answer + Ground Truth + Rubric, WITHOUT Agent Response)
        generated_images: List of PIL Images generated by tool execution
        input_image_path: Path to input image
        question: The original question text (optional, for better organization)
        response_text: The agent's full response text (will be inserted before generated images)
        max_images: Maximum number of generated images to include (default: 3)

    Returns:
        str or list: If no images, returns text. If images, returns list of content dicts in OpenAI format
    """
    # If no images, return text as-is
    if not generated_images and not input_image_path:
        return text_prompt

    # Build content array in OpenAI format with clear organization
    content = []

    # Section 1: Question + Input Image (grouped together)
    if question and input_image_path:
        content.append({
            "type": "input_text",
            "text": f"## Question:\n{question}\n\n**Input Image (original from question):**"
        })
        base64_img = _encode_image_to_base64(input_image_path)
        if base64_img:
            content.append({
                "type": "input_image",
                "image_url": f"data:image/jpeg;base64,{base64_img}"
            })
        content.append({
            "type": "input_text",
            "text": "\n---\n"
        })
    elif input_image_path:
        # Fallback: Just show input image with label
        content.append({
            "type": "input_text",
            "text": "**Input Image (original from question):**"
        })
        base64_img = _encode_image_to_base64(input_image_path)
        if base64_img:
            content.append({
                "type": "input_image",
                "image_url": f"data:image/jpeg;base64,{base64_img}"
            })
        content.append({
            "type": "input_text",
            "text": "\n---\n"
        })

    # Section 2: Agent Response (text from agent's full output)
    if response_text:
        content.append({
            "type": "input_text",
            "text": f"## Agent Response:\n{response_text}\n\n---\n"
        })

    # Section 3: Generated Images (visual outputs from agent's code execution)
    if generated_images:
        content.append({
            "type": "input_text",
            "text": f"**Agent Generated Images ({min(len(generated_images), max_images)} images from code execution):**"
        })
        for idx, img in enumerate(generated_images[:max_images], 1):
            if img is None:
                continue
            base64_img = _encode_image_to_base64(img)
            if base64_img:
                content.append({
                    "type": "input_image",
                    "image_url": f"data:image/jpeg;base64,{base64_img}"
                })
        content.append({
            "type": "input_text",
            "text": "\n---\n"
        })
    else:
        # No images generated - show this explicitly
        content.append({
            "type": "input_text",
            "text": "**No Agent Generated Images**\n\n---\n"
        })

    # Section 4: Evaluation criteria (Agent Answer + Ground Truth + Rubric)
    content.append({
        "type": "input_text",
        "text": text_prompt
    })

    # If no images were successfully added, return text only
    if len([c for c in content if c["type"] == "input_image"]) == 0:
        return text_prompt

    return content


def extract_answer(text):
    """
    Extract the LAST <answer></answer> tag content from text using split method.
    This ensures consistency across all extraction points.

    Args:
        text (str): Text containing <answer> tags

    Returns:
        str or None: Content inside the last <answer> tag, or None if not found
    """
    # Remove everything before </think> to focus on the final response
    predict_no_think = text.split('</think>')[-1].strip()

    # Check if answer tag exists
    if predict_no_think.count("<answer>") == 0:
        return None

    # Extract LAST answer using split method (consistent with line 561)
    answer_parts = predict_no_think.split("<answer>")
    if len(answer_parts) > 1:
        answer_with_closing = answer_parts[-1]
        # Check if closing tag exists
        if "</answer>" in answer_with_closing:
            return answer_with_closing.split("</answer>")[0].strip()
        else:
            # No closing tag - incomplete answer
            return None

    return None


def rule_math_verify(ground_truth, model_answer):
    # Use parsing_timeout=None and timeout_seconds=None to avoid signal.alarm()
    # which doesn't work in multi-threaded environments
    gold = parse(ground_truth, parsing_timeout=None)
    answer = parse(model_answer, parsing_timeout=None)
    return verify(gold, answer, timeout_seconds=None)


GENERATIVE_VERIFY_PROMPT = """
Your job is to assess whether the **Student Answer** captures the strictly same meaning as the **Reference Answer**, even when expressed with different wording or format.
# ATTENTION #
 - The reference answer is ALWAYS correct. You should carefully judge whether the student gives the same answer as reference answer.
 - Respond with one word: TRUE or FALSE
 - Don't give extra explanation.
**Question**:
{query}
**Reference Answer**
{gold_ans}
**Student Answer**
{pred_ans}"""

# System prompts for rubrics judge (task-specific)
RUBRICS_JUDGE_SYSTEM_PROMPT_VS = """You are an expert judge to score a Agent Response's reasoning and tool use quality.

Agent respond with <think>, <code>, <answer>. If code runs sucessfully, sandbox text output is in <sandbox_output>, image output will apear after **Agent Generated Images**

CRITICAL EVALUATION ORDER:
1. RED FLAGS (score = 0 directly):
   - No python <code> or <code> contains only print() and comments
   - Code doesn't perform image operations (crop/resize/load)
   - Overly complex code that doesn't align with the question (reward hacking)

2. If no RED FLAGS:
   - Score = 1 if at least one **Agent Generated Images** contains the CORRECT region/object mentioned in the question
   - Score = 0.75 if **Agent Generated Images** shows a PARTIAL match
   - Score = 0.25 if None of the **Agent Generated Images** matches (Likely reward hacking)
   
3. If no RED FLAGS and no images generated:
   - Score = 0.5 if clear <code> for image processing: load → crop → save.
   - Score = 0.5 If there is error in sandbox_output but fixed it with correct the code later.
   - Score = 0.25 if no image operations with pure text reasoning, reasoning didn't change after seeing sandbox errors.
"""

RUBRICS_JUDGE_SYSTEM_PROMPT_CALCULATION = """You are an expert judge to score a Agent Response's reasoning and tool use quality.

Agent respond with <think>, <code>, <answer>. If code runs sucessfully, sandbox text output is in <sandbox_output>, image output will apear after **Agent Generated Images**

CRITICAL EVALUATION ORDER:
1. RED FLAGS (score = 0 directly):
   - Code is just print(answer) without any calculation/parsing/processing
   - Code has many operations but none are relevant to solving this specific problem (reward hacking)
   - Reasoning is completely wrong or fabricated
   - Overly complex code that doesn't align with the question

2. If no RED FLAGS:
   - Score = 1.0 if strong logical reasoning (code optional - good reasoning without code = high score)
   - Score = 0.75 if code shows genuine calculations AND sandbox shows meaninful outputs instead of errors.
   - Score = 0.5 if partial reasoning or computation
   - Score = 0.5 If there is error in sandbox_output but fixed it with correct the code later.
   - Score = 0.25 if reasoning is weak, code (if any) doesn't help, reasoning didn't change after seeing sandbox errors.

3. ANTI-HACKING CHECK:
   - If code is complex: Does sandbox output justify the complexity?
   - If sandbox only shows final answer with no intermediate steps → likely fake (score ≤ 0.25)

You must output ONLY a single numeric score (0, 0.25, 0.5, 0.75, 1.0). No explanation."""

RUBRICS_JUDGE_SYSTEM_PROMPT_ICON = """You are an expert judge to score a  Agent Response's counting/detection tasks.

Agent respond with <think>, <code>, <answer>. If code runs sucessfully, sandbox text output is in <sandbox_output>, image output will apear after **Agent Generated Images**

CRITICAL: Systematic counting with code is REQUIRED for high scores.

CRITICAL EVALUATION ORDER:
1. RED FLAGS (score = 0 directly):
   - No python <code> (manual visual counting not allowed)
   - Code is just print(count) without detection/counting logic
   - Overly complex code but sandbox doesn't show detection steps (reward hacking)
   - Code has many operations but none actually help count objects

2. If no RED FLAGS:
   - Score = 1.0 if systematic counting code AND sandbox shows clear detection steps
   - Score = 0.75 if good counting approach with reasonable sandbox output
   - Score = 0.5 if code attempts counting but incomplete.
   - Score = 0.5 If there is error in sandbox_output but fixed it with correct the code later.
   - Score = 0.25 If reasoning didn't change after seeing sandbox errors.

3. ANTI-HACKING CHECK (CRITICAL):
   - If code is complex: Does sandbox justify it? (e.g., "Found X regions", "Detected Y objects")
   - If sandbox only shows final count with no intermediate steps → likely fake (score = 0)
   - Be suspicious: Complex filtering/thresholding but vague sandbox output = reward hacking

You must output ONLY a single numeric score (0, 0.25, 0.5, 0.75, 1.0). No explanation."""

# Rubrics templates (user prompts)
RUBRIC_VS_USAGE = """
You must output ONLY a single numeric score (0, 0.25, 0.5, 0.75, 1.0). No explanation.
"""

RUBRIC_CALCULATION_USAGE = """

You must output ONLY a single numeric score (0, 0.25, 0.5, 0.75, 1.0). No explanation."""

RUBRIC_ICON_USAGE = """

You must output ONLY a single numeric score (0, 0.25, 0.5, 0.75, 1.0). No explanation."""


def rubrics_judge(response_str: str, question: str, ground_truth: str, rubric_template: str,
                  extracted_answer: str, system_prompt: str, generated_images=None, input_image_path=None) -> float:
    """
    General rubrics judge that evaluates response based on provided rubric template.

    Args:
        response_str: The agent's full response including <think> and <answer> sections
        question: The original question/task
        ground_truth: The ground truth answer
        rubric_template: The rubric template string with placeholders for response, question, ground_truth, final_answer
        extracted_answer: The already-extracted answer (for consistency)
        system_prompt: Task-specific system prompt (VS/CALCULATION/ICON)
        generated_images: List of PIL Images generated by tool execution (e.g., cropped images)
        input_image_path: Path to the input image

    Returns:
        float: Score from 0.0 to 1.0 based on the rubric
    """
    # Clean & trim response
    cleaned = response_str.replace("<|image_pad|>", "")
    cleaned = cleaned[-6000:]  # Keep last 6000 chars to fit in context

    # Use the already-extracted answer for consistency
    final_answer = extracted_answer if extracted_answer else ""

    # Extract code blocks explicitly
    code_blocks = re.findall(r'<code>(.*?)</code>', cleaned, re.DOTALL)
    if code_blocks:
        code_text = "\n\n---\n\n".join([f"```\n{code.strip()}\n```" for code in code_blocks])
    else:
        code_text = "None - no code was used"

    # Extract sandbox output explicitly
    sandbox_blocks = re.findall(r'<sandbox_output>(.*?)</sandbox_output>', cleaned, re.DOTALL)
    if sandbox_blocks:
        sandbox_text = "\n\n---\n\n".join([block.strip() for block in sandbox_blocks])
    else:
        sandbox_text = "None - code was not executed"

    # Format the rubric prompt with actual values
    user_prompt = rubric_template.format(
        response=cleaned,
        question=(question or "")[:500],
        ground_truth=str(ground_truth or "NULL")[:200],
        final_answer=str(final_answer)[:200],
        code_used=code_text[:1000],  # Limit code length
        sandbox_output=sandbox_text[:1000],  # Limit sandbox length
    )

    client = client_list[0]
    model_name = model_name_list[0]
    try:
        # Prepare input content (text + optional images with clear structure)
        # Order: Question → Input Image → Agent Response → Generated Images → Rubric Template
        input_content = _prepare_multimodal_input(
            text_prompt=user_prompt,
            generated_images=generated_images,
            input_image_path=input_image_path,
            question=question,
            response_text=cleaned,  # Agent's full response (inserted before generated images)
        )

        # Check if input_content contains images (multimodal)
        has_images = isinstance(input_content, list) and any(
            isinstance(item, dict) and item.get('type') == 'input_image'
            for item in input_content
        )

        if has_images:
            # Use chat.completions.create() for multimodal content
            # Convert input_content format from responses to chat API
            chat_content = []
            for item in input_content:
                if item.get('type') == 'input_text':
                    chat_content.append({"type": "text", "text": item['text']})
                elif item.get('type') == 'input_image':
                    chat_content.append({
                        "type": "image_url",
                        "image_url": {"url": item['image_url']}
                    })

            chat_response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": chat_content},
                ],
                temperature=0.7,
            )
            output = chat_response.choices[0].message.content.strip()
        else:
            # Use responses.create() for text-only (faster with reasoning)
            chat_response = client.responses.create(
                model=model_name,
                input=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": input_content},
                ],
                temperature=1,
                reasoning={
                    "effort": "low",
                },
            )
            output = chat_response.output_text.strip()

        # Extract numeric score from output
        # Handle various formats: "0.75", "Score: 0.75", "0.75/1.0", etc.
        score_match = re.search(r'(\d+\.?\d*)', output)
        if score_match:
            score = float(score_match.group(1))
            # Clamp score to [0.0, 1.0] range
            score = max(0.0, min(1.0, score))
            return score
        else:
            print(f"[ERROR] Could not parse score from output: {output}")
            return 0.0

    except Exception as e:
        print(f"[ERROR] Rubrics judgment failed: {e}")
        return 0.0


def generative_verify(query, ground_truth, model_answer):
    client_idx = random.randint(0, len(client_list) - 1)
    client = client_list[client_idx]
    model_name = model_name_list[client_idx]

    full_prompt = GENERATIVE_VERIFY_PROMPT.format(
        query=query,
        gold_ans=ground_truth,
        pred_ans=model_answer,
    )

    response = ""
    for it in range(5):
        try:
            chat_response = client.responses.create(
                model=model_name,
                input=[
                    {"role": "user", "content": full_prompt},
                ],
                temperature=1.0,
                reasoning = {
                  "effort": "minimal"
                },
            )
            response = chat_response.output_text.strip()
            break
        except Exception as e:
            print(f' [ERROR math] generative_verify error: {e}')
            continue
    
    judgement = response.split('## Equivalence Judgement')[-1].lower()
    if 'true' in judgement and 'false' not in judgement:
        return True
    elif 'false' in judgement and 'true' not in judgement:
        return False
    else:
        print(f' [ERROR math] verify bug output: {repr(response)}')
        print(f' [ERROR math] ground_truth: {repr(ground_truth)}')
        print(f' [ERROR math] model_answer: {repr(model_answer)}')
        return False


def check_format_errors(predict_str: str) -> tuple[bool, int]:
    """
    Check for format errors in the prediction string.

    Returns:
        tuple: (is_format_error: bool, code_count: int)
    """
    is_format_error = False

    # Check for mismatched <code> tags
    count_code_open = predict_str.count("<code>")
    count_code_close = predict_str.count("</code>")
    if count_code_open != count_code_close:
        is_format_error = True

    # Check for mismatched <think> tags
    count_think_open = predict_str.count("<think>")
    count_think_close = predict_str.count("</think>")
    if count_think_open != count_think_close:
        is_format_error = True

    # Check for mismatched vision tags
    count_vision_start = predict_str.count("<|vision_start|><|image_pad|>")
    count_vision_end = predict_str.count("<|image_pad|><|vision_end|>")
    if count_vision_start != count_vision_end:
        is_format_error = True

    # Check for code after </answer>
    predict_after_answer = predict_str.split('</answer>')[-1].strip()
    if predict_after_answer.count("<code>") > 0:
        is_format_error = True

    # Check for mismatched <answer> tags (excluding <think> section)
    predict_no_think = predict_str.split('</think>')[-1].strip()
    count_answer_open = predict_no_think.count("<answer>")
    count_answer_close = predict_no_think.count("</answer>")
    if count_answer_open != count_answer_close:
        is_format_error = True

    return is_format_error, count_code_open


def extract_and_verify_answer(predict_str: str, ground_truth: str, extra_info: dict,
                              extracted_answer: str = None) -> tuple[float, bool]:
    """
    Verify answer against ground truth using already-extracted answer or extract if needed.

    Cascade logic:
    1. Use provided extracted_answer if available
    2. Try to extract from \boxed{} pattern as fallback
    3. Verify using rule-based math first, then LLM judge

    Args:
        predict_str: Full prediction string
        ground_truth: Ground truth answer
        extra_info: Extra information including question
        extracted_answer: Already-extracted answer (from extract_answer() function)

    Returns:
        tuple: (acc_reward: float, has_format_error: bool)
    """
    predict_no_think = predict_str.split('</think>')[-1].strip()

    # Step 1: Use provided extracted_answer if available
    if extracted_answer is not None:
        model_answer = extracted_answer
        # Check for format errors: multiple answer tags
        has_format_error = predict_no_think.count("<answer>") > 1
    else:
        # Step 2: Fallback - try to extract from \boxed{} pattern
        answer_pattern = r'\\boxed{([^}]+)}'
        answer_list = re.findall(answer_pattern, predict_no_think, flags=re.DOTALL)

        if len(answer_list) > 0:
            # Found \boxed{} pattern
            model_answer = answer_list[-1]
            has_format_error = len(answer_list) > 1  # Multiple \boxed{} is a format error
        else:
            # No answer found
            return 0.0, True

    # Step 3: Verify answer using unified verification logic
    # Try rule-based math verification first (handles exact match as well)
    if rule_math_verify(ground_truth, model_answer):
        return 1.0, has_format_error

    # Fall back to LLM judge
    acc = 1.0 if generative_verify(extra_info['question'], ground_truth, model_answer) else 0.0
    return acc, has_format_error


def compute_score_generative(predict_str: str, ground_truth: str, extra_info=None, data_source=None, generated_images=None) -> float:
    """
    Compute reward score based on accuracy, format, and tool usage.

    Shared components (single function calls):
    - extract_answer(): Extract answer once for consistency
    - check_format_errors(): Check format validity
    - extract_and_verify_answer(): Verify answer correctness

    Customizable per data source:
    - compute_process_reward_*(): Different process reward functions per data source
    """
    # Step 0: Extract answer once (used by both verification and judge for consistency)
    extracted_answer = extract_answer(predict_str)

    # Step 1: Check format errors (shared utility)
    is_format_error, code_count = check_format_errors(predict_str)

    # Step 2: Verify answer (shared utility, uses extracted_answer)
    acc_reward, has_answer_format_error = extract_and_verify_answer(
        predict_str, ground_truth, extra_info, extracted_answer=extracted_answer
    )

    # Combine format errors
    is_format_error = is_format_error or has_answer_format_error
    format_reward = 0.0 if is_format_error else 1.0

    # Step 3: Compute process reward (customizable per data source)
    if extra_info and 'question' in extra_info:
        # Get input image path if available
        input_image_path = extra_info.get('image_file_path', None)

        # Dispatch to different process reward functions based on data_source
        if data_source in ['vstar','chart','high_res']:
            process_reward = rubrics_judge(
                predict_str, extra_info['question'], ground_truth, RUBRIC_VS_USAGE,
                extracted_answer=extracted_answer or "",
                system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_VS,
                generated_images=generated_images, input_image_path=input_image_path
            )
        elif data_source in ['GeoQA','Geometry3K','TabMWP']:
            # Geometry, OCR, and similar tasks: Python as calculator encouraged but optional
            process_reward = rubrics_judge(
                predict_str, extra_info['question'], ground_truth, RUBRIC_CALCULATION_USAGE,
                extracted_answer=extracted_answer or "",
                system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_CALCULATION,
                generated_images=generated_images, input_image_path=input_image_path
            )
        elif data_source == 'IconQA':
            process_reward = rubrics_judge(
                predict_str, extra_info['question'], ground_truth, RUBRIC_ICON_USAGE,
                extracted_answer=extracted_answer or "",
                system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_ICON,
                generated_images=generated_images, input_image_path=input_image_path
            )
        else:
            # Default: use calculation rubric
            process_reward = rubrics_judge(
                predict_str, extra_info['question'], ground_truth, RUBRIC_CALCULATION_USAGE,
                extracted_answer=extracted_answer or "",
                system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_CALCULATION,
                generated_images=generated_images, input_image_path=input_image_path
            )
    else:
        assert data_source is not None, "data_source must be provided if extra_info is missing question"
        process_reward = 0.0

    # Step 4: Combine rewards (can be customized per data source)
    total_reward = 1.0 * acc_reward + 0.3 * format_reward + 0.5 * process_reward

    return {
        "score": total_reward,
        "acc": acc_reward,
        "format": format_reward,
        "tool": process_reward,
    }


def compute_score_generative_batched(data_sources, solution_strs, ground_truths, extra_infos,
                                     generated_images=None, num_workers=10):
    """
    Batched version of compute_score_generative with parallel execution.
    Uses ThreadPoolExecutor to parallelize LLM-as-a-judge API calls.
    This function is designed to work with BatchRewardManager which calls compute_score
    with lists of items instead of individual items.
    Args:
        data_sources: List of data source names (passed to compute_score_generative for custom rewards)
        solution_strs: List of model response strings
        ground_truths: List of ground truth answers
        extra_infos: List of extra info dicts
        generated_images: List of lists of PIL Images (one list per sample)
        num_workers: Number of parallel workers (default: 10, tune based on API rate limits)
    Returns:
        List of score dicts, one per input
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    # Handle missing generated_images
    if generated_images is None:
        generated_images = [[]] * len(solution_strs)

    def compute_single(idx, data_source, solution_str, ground_truth, extra_info, gen_images):
        """Wrapper to preserve index for result ordering."""
        try:
            score = compute_score_generative(solution_str, ground_truth, extra_info,
                                            data_source=data_source, generated_images=gen_images)
            return idx, score
        except Exception as e:
            print(f"Error computing score for item {idx}: {e}")
            # Return a default score on error
            return idx, {"score": 0.0, "acc": 0.0, "format": 0.0, "tool": 0.0}

    results = [None] * len(solution_strs)

    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        # Submit all tasks
        futures = []
        for i in range(len(solution_strs)):
            data_source = data_sources[i] if i < len(data_sources) else data_sources[0]
            gen_images = generated_images[i] if i < len(generated_images) else []
            future = executor.submit(compute_single, i, data_source, solution_strs[i],
                                   ground_truths[i], extra_infos[i], gen_images)
            futures.append(future)

        # Collect results as they complete
        for future in as_completed(futures):
            idx, score = future.result()
            results[idx] = score

    return results


# ============================================================================
# TEST SUITE
# ============================================================================

def test_answer_extraction():
    """Test answer extraction from various response formats."""
    print("\n" + "=" * 80)
    print("TEST: Answer Extraction")
    print("=" * 80)

    test_cases = [
        ("The answer is <answer>42</answer>", "42"),
        ("<think>reasoning</think><answer>left</answer>", "left"),
        ("Multiple <answer>wrong</answer> answers <answer>correct</answer>", "correct"),
        ("<answer> spaced </answer>", "spaced"),
        ("No answer tag", None),
    ]

    passed = 0
    for text, expected in test_cases:
        result = extract_answer(text)
        status = "PASS" if result == expected else "FAIL"
        print(f"{status} Input: {text[:50]}...")
        print(f"  Expected: {expected}, Got: {result}")
        if result == expected:
            passed += 1

    print(f"\nPassed: {passed}/{len(test_cases)}")
    return passed == len(test_cases)


def test_fake_code_detection():
    """Test that rubrics properly penalize fake/meaningless code."""
    print("\n" + "=" * 80)
    print("TEST: Fake Code Detection (Should Score Low)")
    print("=" * 80)

    question = "how many shape in blue"
    ground_truth = "4"
    image_path = "/workspace/shared/datasets/CodeV_images/1054.jpg"

    # Case 1: Fake code that just prints the answer
    fake_response = """<think>Looking at the image, I can count the blue shapes. I see 4 blue shapes in total.</think>
<code>print('<answer>4</answer>')</code>
<sandbox_output>4</sandbox_output>
<answer>4</answer>"""

    extracted = extract_answer(fake_response)
    print(f"Question: {question}")
    print(f"Image: {image_path}")
    print(f"Testing fake code: print('<answer>4</answer>')")
    print(f"Extracted answer: {extracted}")

    score = rubrics_judge(
        response_str=fake_response,
        question=question,
        ground_truth=ground_truth,
        rubric_template=RUBRIC_ICON_USAGE,  # Use ICON_USAGE for counting tasks
        extracted_answer=extracted,
        system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_ICON,
        generated_images=None,
        input_image_path=image_path
    )
    print(f"Rubric score: {score:.2f}")
    print(f"Expected: Low score (≤0.25) for fake code")
    print(f"Result: {'PASS' if score <= 0.25 else 'FAIL'}")

    return score <= 0.25


def test_helpful_code():
    """Test that rubrics reward helpful code."""
    print("\n" + "=" * 80)
    print("TEST: Helpful Code (Should Score High)")
    print("=" * 80)

    question = "how many shape in blue"
    ground_truth = "4"
    image_path = "/workspace/shared/datasets/CodeV_images/1054.jpg"

    helpful_response = """<think>I need to load the image and count blue shapes systematically using image processing.</think>
<code>
from PIL import Image
import numpy as np

img = Image.open('/workspace/shared/datasets/CodeV_images/1054.jpg')
img_array = np.array(img)

# Extract blue channel and threshold
blue_mask = (img_array[:,:,2] > 150) & (img_array[:,:,0] < 100) & (img_array[:,:,1] < 100)

# Count connected components
from scipy import ndimage
labeled, num_features = ndimage.label(blue_mask)
print(f"Number of blue shapes detected: {num_features}")
</code>
<sandbox_output>Number of blue shapes detected: 4</sandbox_output>
<answer>4</answer>"""

    extracted = extract_answer(helpful_response)
    print(f"Question: {question}")
    print(f"Image: {image_path}")
    print(f"Testing helpful code: Image processing with blue detection")
    print(f"Extracted answer: {extracted}")

    score = rubrics_judge(
        response_str=helpful_response,
        question=question,
        ground_truth=ground_truth,
        rubric_template=RUBRIC_ICON_USAGE,  # Use ICON_USAGE for counting
        extracted_answer=extracted,
        system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_ICON,
        generated_images=None,
        input_image_path=image_path
    )
    print(f"Rubric score: {score:.2f}")
    print(f"Expected: High score (≥0.5) for helpful code")
    print(f"Result: {'PASS' if score >= 0.5 else 'FAIL'}")

    return score >= 0.5


def test_no_code_response():
    """Test responses without any code."""
    print("\n" + "=" * 80)
    print("TEST: No Code Response (Should Score Low)")
    print("=" * 80)

    question = "how many shape in blue"
    ground_truth = "4"
    image_path = "/workspace/shared/datasets/CodeV_images/1054.jpg"

    no_code_response = """<think>Looking at the image, I can visually count the blue shapes. I see a blue circle, a blue square, a blue triangle, and a blue rectangle. That's 4 blue shapes in total.</think>
<answer>4</answer>"""

    extracted = extract_answer(no_code_response)
    print(f"Question: {question}")
    print(f"Image: {image_path}")
    print(f"Testing response without code")
    print(f"Extracted answer: {extracted}")

    # Use ICON_USAGE which requires code for scores above 0.25
    score = rubrics_judge(
        response_str=no_code_response,
        question=question,
        ground_truth=ground_truth,
        rubric_template=RUBRIC_ICON_USAGE,
        extracted_answer=extracted,
        system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_ICON,
        generated_images=None,
        input_image_path=image_path
    )
    print(f"Rubric score (ICON_USAGE): {score:.2f}")
    print(f"Expected: Low score (≤0.25) since counting requires code")
    print(f"Result: {'PASS' if score <= 0.25 else 'FAIL'}")

    return score <= 0.25


def test_rubrics_with_generated_images():
    """Test rubrics evaluation with generated images."""
    print("\n" + "=" * 80)
    print("TEST: Rubrics with Generated Images")
    print("=" * 80)

    question = "Crop and show the blue circle from the image"
    ground_truth = "blue circle"
    image_path = "/workspace/shared/datasets/CodeV_images/1054.jpg"

    response_with_images = """<think>I need to identify the blue circle's location and crop that region from the image.</think>
<code>
from PIL import Image
img = Image.open('/workspace/shared/datasets/CodeV_images/1054.jpg')
# Blue circle is approximately at coordinates (50, 50, 150, 150)
cropped = img.crop((50, 50, 150, 150))
cropped.save('blue_circle.jpg')
print("Cropped blue circle region")
</code>
<sandbox_output>Cropped blue circle region</sandbox_output>
<answer>blue circle</answer>"""

    extracted = extract_answer(response_with_images)
    print(f"Question: {question}")
    print(f"Image: {image_path}")
    print(f"Testing with mock generated images")

    # Create mock PIL images
    try:
        from PIL import Image
        import io
        # Create a small dummy image
        img = Image.new('RGB', (100, 100), color='red')
        mock_images = [img]
    except:
        print("Warning: PIL not available, testing without images")
        mock_images = None

    score = rubrics_judge(
        response_str=response_with_images,
        question=question,
        ground_truth=ground_truth,
        rubric_template=RUBRIC_VS_USAGE,
        extracted_answer=extracted,
        system_prompt=RUBRICS_JUDGE_SYSTEM_PROMPT_VS,
        generated_images=mock_images,
        input_image_path=image_path
    )
    print(f"Rubric score (VS_USAGE): {score:.2f}")
    print(f"Expected: Should recognize code + execution + generated images")
    print(f"Result: Test completed (score: {score:.2f})")

    return True  # Just verify it runs without error


def test_math_verify():
    """Test mathematical answer verification."""
    print("\n" + "=" * 80)
    print("TEST: Math Verification")
    print("=" * 80)

    test_cases = [
        ("42", "42", True),
        ("3.14159", "3.14", True),
        ("0.5", "1/2", True),
        ("2", "3", False),
    ]

    passed = 0
    for pred, gt, expected in test_cases:
        try:
            result = rule_math_verify(pred, gt)
            status = "PASS" if result == expected else "FAIL"
            print(f"{status} rule_math_verify('{pred}', '{gt}'): {result} (expected: {expected})")
            if result == expected:
                passed += 1
        except Exception as e:
            print(f"FAIL Error: {e}")

    print(f"\nPassed: {passed}/{len(test_cases)}")
    return passed == len(test_cases)


def test_batched_scoring():
    """Test parallel batched scoring."""
    print("\n" + "=" * 80)
    print("TEST: Batched Parallel Scoring")
    print("=" * 80)

    data_sources = ['vstar', 'vstar', 'vstar']
    solution_strs = [
        "<answer>left</answer>",
        "<answer>1024</answer>",
        "<answer>42</answer>"
    ]
    ground_truths = ["left", "1024", "42"]
    extra_infos = [
        {'question': 'Which side?', 'answer': 'left'},
        {'question': 'What width?', 'answer': '1024'},
        {'question': 'What number?', 'answer': '42'}
    ]

    import time
    print("Running batched scoring with 3 workers...")
    start = time.time()
    try:
        results = compute_score_generative_batched(
            data_sources, solution_strs, ground_truths, extra_infos, num_workers=3
        )
        elapsed = time.time() - start
        print(f"Completed in {elapsed:.2f}s")
        for i, result in enumerate(results):
            print(f"  Result {i}: {result}")
        return True
    except Exception as e:
        print(f"Error: {e}")
        return False


def test_rubric_templates():
    """Test all rubric template types."""
    print("\n" + "=" * 80)
    print("TEST: All Rubric Templates")
    print("=" * 80)

    rubrics = [
        ("VS_USAGE", RUBRIC_VS_USAGE, RUBRICS_JUDGE_SYSTEM_PROMPT_VS, "Visual Search"),
        ("CALCULATION_USAGE", RUBRIC_CALCULATION_USAGE, RUBRICS_JUDGE_SYSTEM_PROMPT_CALCULATION, "Calculation/Reasoning"),
        ("ICON_USAGE", RUBRIC_ICON_USAGE, RUBRICS_JUDGE_SYSTEM_PROMPT_ICON, "Counting"),
    ]

    question = "Test question"
    ground_truth = "test"
    response = """<think>Testing</think>
<code>print('test')</code>
<sandbox_output>test</sandbox_output>
<answer>test</answer>"""
    extracted = extract_answer(response)

    passed = 0
    for name, template, system_prompt, description in rubrics:
        try:
            score = rubrics_judge(
                response_str=response,
                question=question,
                ground_truth=ground_truth,
                rubric_template=template,
                extracted_answer=extracted,
                system_prompt=system_prompt,
                generated_images=None,
                input_image_path=None
            )
            print(f"PASS {name} ({description}): {score:.2f}")
            passed += 1
        except Exception as e:
            print(f"FAIL {name} ({description}): Error - {e}")

    print(f"\nPassed: {passed}/{len(rubrics)}")
    return passed == len(rubrics)


def run_all_tests():
    """Run all test cases."""
    import os

    # Check for API key
    if not os.environ.get('OPENAI_API_KEY'):
        print("\n" + "!" * 80)
        print("WARNING: OPENAI_API_KEY not set!")
        print("Please run: export OPENAI_API_KEY='your-api-key'")
        print("Or: OPENAI_API_KEY='your-api-key' python vl_agent.py")
        print("!" * 80)
        return

    print("\n" + "=" * 80)
    print("RUNNING VL_AGENT TEST SUITE")
    print("=" * 80)

    results = {}

    # Run tests that don't require API calls
    results['Answer Extraction'] = test_answer_extraction()

    # Run tests that require API calls
    print("\nTests requiring OpenAI API calls:")
    try:
        results['Fake Code Detection'] = test_fake_code_detection()
        results['Helpful Code'] = test_helpful_code()
        results['No Code Response'] = test_no_code_response()
        results['Generated Images'] = test_rubrics_with_generated_images()
        results['Math Verify'] = test_math_verify()
        results['Batched Scoring'] = test_batched_scoring()
        results['Rubric Templates'] = test_rubric_templates()
    except Exception as e:
        print(f"\nTest suite error: {e}")
        import traceback
        traceback.print_exc()

    # Summary
    print("\n" + "=" * 80)
    print("TEST SUMMARY")
    print("=" * 80)
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    for test_name, result in results.items():
        status = "PASS" if result else "FAIL"
        print(f"{status}: {test_name}")

    print(f"\nTotal: {passed}/{total} passed")
    print("=" * 80)


if __name__ == '__main__':
    run_all_tests()
